import streamlit as st
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
import os

AIzaSyAr5dCJwr-
JQf22us1pzbN2ktHKvf6zOTQ


AIzaSyBH5LSI4BdiXsclWLVQ5E4
-ovjgAm0qhLM



# --- API Key Configuration ---
# For demonstration, we'll use a sidebar input.
# For production, it's highly recommended to use st.secrets.
# GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
st.sidebar.title("Configuration")
api_key_input = st.sidebar.text_input("Enter your Google API Key", type="password")

# It's better to set the API key as an environment variable
if api_key_input:
    os.environ["GOOGLE_API_KEY"] = api_key_input
else:
    # A placeholder message if the key is not entered
    st.warning("Please enter your Google API Key in the sidebar to proceed.")
    st.stop()


# --- Main App ---
st.header("Chat with your PDF using Gemini Pro")

# Upload PDF files
with st.sidebar:
    st.title("Your Documents")
    file = st.file_uploader("Upload a PDF file and start asking questions", type="pdf")

# Extract the text
if file is not None:
    pdf_reader = PdfReader(file)
    text = ""
    for page in pdf_reader.pages:
        text += page.extract_text() or "" # Added 'or ""' to handle possible None return

    # Break it into chunks
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=10000,
        chunk_overlap=1000,
        length_function=len
    )
    chunks = text_splitter.split_text(text)

    # --- Use GoogleGenerativeAIEmbeddings ---
    # generating embedding
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001") # Default model

    # creating vector store - FAISS
    vector_store = FAISS.from_texts(chunks, embeddings)

    # get user question
    user_question = st.text_input("Ask a question about your PDF:")

    # do similarity search
    if user_question:
        docs = vector_store.similarity_search(user_question)

        # --- Use ChatGoogleGenerativeAI ---
        # define the LLM
        llm = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0.3)

        # Define a prompt template
        prompt_template = """
        Answer the question as detailed as possible from the provided context. If the answer is not in
        the provided context, just say, "The answer is not available in the context". Don't provide a wrong answer.\n\n
        Context:\n {context}\n
        Question: \n{question}\n

        Answer:
        """
        prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

        # output results
        # chain -> take the question, get relevant document, pass it to the LLM, generate the output
        chain = load_qa_chain(llm, chain_type="stuff", prompt=prompt)
        response = chain({"input_documents": docs, "question": user_question}, return_only_outputs=True)
        
        st.write("### Answer")
        st.write(response["output_text"])
